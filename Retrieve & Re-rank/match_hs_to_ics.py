import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer, CrossEncoder
import torch
import os
import re

# ==========================================
# âš™ï¸ ç©¶æç‰ˆé…ç½®
# ==========================================
HS_FILE_PATH = "HS07.xlsx - Sheet1.csv"
ICS_FILE_PATH = "Data_ics_ed7.xlsx - Sheet1.csv"
# ä¿®æ”¹ï¼šç›´æ¥è¾“å‡ºä¸º xlsx æ ¼å¼ï¼Œæ–¹ä¾¿ä¸»äººæŸ¥çœ‹å–µï¼
OUTPUT_FILE = "HS_to_ICS_Ultimate_Match.xlsx"

# åˆç­›æ•°é‡ï¼šç¬¬ä¸€æ­¥å…ˆæ‰¾å›å¤šå°‘ä¸ªå€™é€‰ï¼Ÿ
CANDIDATE_POOL_SIZE = 20  
# æœ€ç»ˆä¿ç•™æ•°é‡
FINAL_TOP_K = 3           

# æƒé‡é…ç½®ï¼ˆåˆç­›é˜¶æ®µï¼‰
ALPHA_SEMANTIC = 0.7 
ALPHA_KEYWORD = 0.3

def robust_read_csv(file_path):
    """
    è‡ªåŠ¨å°è¯•å¤šç§ç¼–ç  + å¤šç§åˆ†éš”ç¬¦è¯»å– CSV çš„èªæ˜å°å·¥å…·å–µ
    (å‡çº§ç‰ˆï¼šä¼˜å…ˆå¯»æ‰¾åˆ—æ•° >= 2 çš„ç»“æœ)
    """
    # å¸¸è§ç¼–ç 
    encodings = ['utf-8', 'gbk', 'utf-16', 'latin1']
    # å¸¸è§åˆ†éš”ç¬¦ï¼šé€—å·ï¼ŒTabï¼Œåˆ†å·
    separators = [',', '\t', ';']
    
    print(f"ğŸ” æ­£åœ¨å°è¯•è¯»å–æ–‡ä»¶: {file_path} ...")
    
    best_df = None
    max_cols = 0
    
    for enc in encodings:
        for sep in separators:
            try:
                # engine='python' å¯¹é”™è¯¯çš„å¤„ç†ç¨å¾®å®½å®¹ä¸€ç‚¹
                df = pd.read_csv(
                    file_path, 
                    header=None, 
                    dtype=str, 
                    encoding=enc, 
                    sep=sep,
                    on_bad_lines='skip', 
                    engine='python'
                )
                
                # å¦‚æœç°åœ¨çš„åˆ—æ•°æ¯”ä¹‹å‰è¯•å‡ºæ¥çš„éƒ½å¤šï¼Œå°±æš‚å­˜è¿™ä¸ªç»“æœ
                if df.shape[1] > max_cols:
                    max_cols = df.shape[1]
                    best_df = df
                    
                # å¦‚æœæ‰¾åˆ°äº† >= 2 åˆ—çš„æ•°æ®ï¼Œè¿™å¾ˆå¯èƒ½å°±æ˜¯å¯¹çš„ï¼Œç›´æ¥è¿”å›ï¼
                if df.shape[1] >= 2:
                    print(f"âœ… æˆåŠŸè¯»å–! ç¼–ç : {enc}, åˆ†éš”ç¬¦: {repr(sep)}, å½¢çŠ¶: {df.shape}")
                    return df
                
            except Exception:
                # å¦‚æœæŠ¥é”™äº†ï¼Œå°±é»˜é»˜å°è¯•ä¸‹ä¸€ä¸ªç»„åˆ
                continue
    
    # å¦‚æœè¯•äº†ä¸€åœˆè¿˜æ˜¯æ‰¾ä¸åˆ° >= 2 åˆ—çš„ï¼Œå°±è¿”å›åˆ—æ•°æœ€å¤šçš„é‚£ä¸ªï¼ˆè™½ç„¶å¯èƒ½åªæœ‰1åˆ—ï¼‰
    if best_df is not None:
        print(f"âš ï¸ è­¦å‘Š: çŒ«çŒ«æ²¡èƒ½æ‰¾åˆ°å®Œç¾çš„æ ¼å¼ï¼Œä½¿ç”¨çš„æ˜¯: å½¢çŠ¶ {best_df.shape}ã€‚å°è¯•åç»­ä¿®å¤...")
        return best_df
                
    raise ValueError(f"ğŸ™€ å‘œå‘œï¼ŒçŒ«çŒ«ç”¨å°½å…¨åŠ›ä¹Ÿæ²¡èƒ½è¯»æ‡‚è¿™ä¸ªæ–‡ä»¶çš„æ ¼å¼: {file_path}")

def fix_one_column_df(df, name="æ•°æ®"):
    """
    å¦‚æœæ•°æ®åªæœ‰1åˆ—ï¼Œå°è¯•æ™ºèƒ½ä¿®å¤
    """
    if df.shape[1] >= 2:
        return df
        
    print(f"ğŸ”§ {name} åªæœ‰1åˆ—ï¼ŒçŒ«çŒ«å°è¯•è¿›è¡Œæ™ºèƒ½åˆ†åˆ—...")
    
    # å°è¯• 1: ç”¨é€—å·æˆ–åˆ†å·æ‹†åˆ†ç¬¬ä¸€åˆ—
    # å‡è®¾æ ¼å¼æ˜¯ "Code;Description" ä½†æ²¡è¢«æ­£ç¡®è§£æ
    try:
        series = df.iloc[:, 0].astype(str)
        # å°è¯•å¸¸è§åˆ†éš”ç¬¦æ‹†åˆ† (expand=True ä¼šå˜æˆå¤šåˆ—)
        for sep in [',', ';', '\t', ' ']:
            split_df = series.str.split(sep, n=1, expand=True)
            if split_df.shape[1] >= 2:
                print(f"   âœ¨ ä½¿ç”¨ '{sep}' æˆåŠŸæ‹†åˆ†!")
                return split_df
    except Exception:
        pass
        
    print(f"   ğŸ’¨ æ‹†åˆ†å¤±è´¥ï¼Œå°†å¤åˆ¶ç¬¬ä¸€åˆ—ä½œä¸ºç¬¬äºŒåˆ—ä»¥é˜²å´©æºƒ...")
    # å®åœ¨ä¸è¡Œï¼Œå°±å¤åˆ¶ä¸€åˆ—ï¼Œé˜²æ­¢ç¨‹åºå´©æºƒ (è™½ç„¶ç»“æœå¯èƒ½ä¸å¤ªå¯¹)
    df['Description_Placeholder'] = df.iloc[:, 0]
    return df

def preprocess_hs_with_context(hs_df):
    """
    ä¸Šä¸‹æ–‡å¢å¼ºï¼šå°† HS çš„ç« èŠ‚æ ‡é¢˜ (2ä½ç¼–ç ) æ‹¼æ¥åˆ° 6ä½ç¼–ç æè¿°å‰ã€‚
    è§£å†³å¾ˆå¤š 6ä½ç¼–ç æè¿°åªæ˜¯ "Other" æˆ– "Parts" çš„é—®é¢˜ã€‚
    """
    print("âœ¨ æ­£åœ¨è¿›è¡Œä¸Šä¸‹æ–‡å¢å¼ºå¤„ç†...")
    
    # ç¡®ä¿è‡³å°‘æœ‰ä¸¤åˆ—
    if hs_df.shape[1] < 2:
        hs_df = fix_one_column_df(hs_df, "HSæ•°æ®")
        
    # å¼ºåˆ¶å–å‰ä¸¤åˆ—
    hs_df = hs_df.iloc[:, :2]
    hs_df.columns = ['HS_Code', 'HS_Description']
    
    # === æ–°å¢ï¼šå¼ºåŠ›æ¸…æ´—é€»è¾‘ ===
    # 1. è½¬ä¸ºå­—ç¬¦ä¸²
    hs_df['HS_Code'] = hs_df['HS_Code'].astype(str)
    # 2. å»é™¤å°æ•°ç‚¹ (å¦‚ 1234.56 -> 123456)
    hs_df['HS_Code'] = hs_df['HS_Code'].str.replace('.', '', regex=False)
    # 3. å»é™¤å‰åç©ºæ ¼
    hs_df['HS_Code'] = hs_df['HS_Code'].str.strip()
    
    hs_df['HS_Description'] = hs_df['HS_Description'].fillna('').str.strip()
    
    # æå– 2ä½æ•° ç« èŠ‚ (Chapter) åŠå…¶æè¿°
    chapters = hs_df[hs_df['HS_Code'].str.len() == 2].set_index('HS_Code')['HS_Description'].to_dict()
    
    # æå– 4ä½æ•° (Heading) åŠå…¶æè¿°
    headings = hs_df[hs_df['HS_Code'].str.len() == 4].set_index('HS_Code')['HS_Description'].to_dict()

    # ç­›é€‰ç›®æ ‡ 6ä½æ•°äº§å“ (åŒ…æ‹¬åŸæœ¬å¤§äº6ä½çš„æˆªå–å‰6ä½)
    # åªè¦é•¿åº¦ >= 6 çš„éƒ½ä¿ç•™
    hs_target = hs_df[hs_df['HS_Code'].str.len() >= 6].copy()
    
    # å¦‚æœæ˜¯8ä½æˆ–æ›´å¤šï¼Œæˆªå–å‰6ä½ä½œä¸ºæ ‡å‡† HS6
    hs_target['HS6_Clean'] = hs_target['HS_Code'].str.slice(0, 6)
    
    enhanced_descriptions = []
    for idx, row in hs_target.iterrows():
        code = row['HS6_Clean'] # ä½¿ç”¨æ¸…æ´—åçš„6ä½ç æ‰¾çˆ¶çº§
        desc = row['HS_Description']
        
        # å®‰å…¨åˆ‡ç‰‡
        chap_code = str(code)[:2]
        head_code = str(code)[:4]
        
        context_str = ""
        if chap_code in chapters:
            context_str += f"{chapters[chap_code]} > "
        if head_code in headings:
            context_str += f"{headings[head_code]} > "
            
        full_desc = f"{context_str}{desc}"
        enhanced_descriptions.append(full_desc)
        
    hs_target['Enhanced_Description'] = enhanced_descriptions
    return hs_target

def run_ultimate_matching():
    print("ğŸ± å¯åŠ¨ç©¶æåŒ¹é…å¼•æ“ (Retrieve & Re-rank)...")
    
    # 1. åŠ è½½æ•°æ® (ä½¿ç”¨é²æ£’è¯»å–)
    hs_df = robust_read_csv(HS_FILE_PATH)
    ics_df = robust_read_csv(ICS_FILE_PATH)
    
    # æ£€æŸ¥å¹¶ä¿®å¤åˆ—æ•°
    if ics_df.shape[1] < 2:
        ics_df = fix_one_column_df(ics_df, "ICSæ•°æ®")
    
    # ICSç«¯å¤„ç†
    # ç¡®ä¿ ICS è‡³å°‘æœ‰3åˆ—ï¼Œå¦‚æœæ²¡æœ‰ï¼Œå°è¯•å…¼å®¹
    if ics_df.shape[1] >= 3:
         ics_df = ics_df.iloc[:, :3]
         ics_df.columns = ['ICS_Code', 'ICS_Description', 'Finest_Level']
    else:
        print("âš ï¸ è­¦å‘Šï¼šICS æ–‡ä»¶å°‘äº3åˆ—ï¼ŒçŒ«çŒ«å°è¯•å¼ºåˆ¶è§£æ...")
        # åªæœ‰2åˆ—çš„æƒ…å†µ
        ics_df = ics_df.iloc[:, :2]
        ics_df.columns = ['ICS_Code', 'ICS_Description']
        ics_df['Finest_Level'] = '1' # å‡å®šå…¨æ˜¯ç»†åˆ†çº§åˆ«
    
    # 2. ä¸Šä¸‹æ–‡å¢å¼º (HSç«¯)
    try:
        hs_target = preprocess_hs_with_context(hs_df)
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"âŒ å¤„ç† HS æ•°æ®æ—¶å‡ºé”™: {e}")
        return
    
    # ICS æ•°æ®æ¸…æ´—
    ics_df['ICS_Description'] = ics_df['ICS_Description'].fillna('').str.strip()
    # ç­›é€‰ç»†åˆ†çº§åˆ«ä¸º 1 çš„æ¡ç›®
    ics_target = ics_df[ics_df['Finest_Level'] == '1'].reset_index(drop=True)
    
    # å¦‚æœç­›é€‰åä¸ºç©ºï¼Œå¯èƒ½æ˜¯ Finest_Level åˆ—ä¸å¯¹ï¼Œå°è¯•æ”¾å®½æ¡ä»¶
    if len(ics_target) == 0:
        print("âš ï¸ è­¦å‘Šï¼šç­›é€‰ Finest_Level='1' åä¸ºç©ºï¼Œå°è¯•ä½¿ç”¨æ‰€æœ‰ ICS æ¡ç›®...")
        ics_target = ics_df
    
    if len(ics_target) == 0:
        print("âŒ é”™è¯¯ï¼šICS ç›®æ ‡åº“ä¸ºç©ºï¼Œæ— æ³•è¿›è¡ŒåŒ¹é…å–µï¼")
        return

    print(f"ğŸ“Š å¾…åŒ¹é… HSæ¡ç›®: {len(hs_target)} (å·²å¢å¼ºä¸Šä¸‹æ–‡)")
    print(f"ğŸ“š ç›®æ ‡ ICSåº“: {len(ics_target)}")

    # ==========================================
    # ğŸš€ Stage 1: å¿«é€Ÿå¬å› (Bi-Encoder + TF-IDF)
    # ==========================================
    print("\n[Stage 1] å¿«é€Ÿå¬å› (Bi-Encoder)...")
    
    # åŠ è½½ Bi-Encoder æ¨¡å‹
    bi_encoder = SentenceTransformer('all-MiniLM-L6-v2')
    
    # ç¼–ç 
    hs_descriptions = hs_target['Enhanced_Description'].tolist()
    ics_descriptions = ics_target['ICS_Description'].tolist()
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
    if not hs_descriptions:
        print("ğŸ™€ å“å‘€ï¼ŒHS æ•°æ®åˆ—è¡¨ä¸ºç©ºï¼Œå¯èƒ½æ˜¯æ¸…æ´—è¿‡ç¨‹æŠŠæ‰€æœ‰æ•°æ®éƒ½è¿‡æ»¤æ‰äº†å–µï¼")
        return

    hs_embeddings = bi_encoder.encode(hs_descriptions, convert_to_tensor=True, show_progress_bar=True)
    ics_embeddings = bi_encoder.encode(ics_descriptions, convert_to_tensor=True, show_progress_bar=True)
    
    # è¯­ä¹‰ç›¸ä¼¼åº¦ (ä½™å¼¦ç›¸ä¼¼åº¦)
    semantic_sim = cosine_similarity(hs_embeddings.cpu(), ics_embeddings.cpu())
    
    # TF-IDF è¾…åŠ© (é’ˆå¯¹ç¡¬æ ¸å…³é”®è¯)
    print("[Stage 1] å…³é”®è¯ä¿®æ­£ (TF-IDF)...")
    tfidf = TfidfVectorizer(stop_words='english')
    corpus = hs_descriptions + ics_descriptions
    tfidf.fit(corpus)
    
    hs_tfidf = tfidf.transform(hs_descriptions)
    ics_tfidf = tfidf.transform(ics_descriptions)
    keyword_sim = cosine_similarity(hs_tfidf, ics_tfidf)
    
    # æ··åˆåˆ†æ•°
    stage1_scores = (semantic_sim * ALPHA_SEMANTIC) + (keyword_sim * ALPHA_KEYWORD)

    # ==========================================
    # ğŸ’ Stage 2: ç²¾ç»†é‡æ’åº (Cross-Encoder)
    # ==========================================
    print("\n[Stage 2] æ·±åº¦é‡æ’åº (Cross-Encoder)... æ­¤æ­¥éª¤è®¡ç®—é‡è¾ƒå¤§ï¼ŒçŒ«çŒ«æ­£åœ¨å…¨åŠ›ä»¥èµ´å–µï¼")
    
    # åŠ è½½ Cross-Encoder æ¨¡å‹
    cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
    
    results = []
    total = len(hs_target)
    
    # éå†æ¯ä¸ª HS äº§å“è¿›è¡Œç²¾ç»†æ‰“åˆ†
    for i in range(total):
        # è·å– Stage 1 åˆ†æ•°æœ€é«˜çš„ä¸€æ‰¹å€™é€‰è€…
        # argsort ä»å°åˆ°å¤§ï¼Œå–æœ€å CANDIDATE_POOL_SIZE ä¸ªå¹¶åè½¬
        candidate_indices = stage1_scores[i].argsort()[-CANDIDATE_POOL_SIZE:][::-1]
        
        hs_text = hs_target.iloc[i]['Enhanced_Description']
        
        # å‡†å¤‡ Cross-Encoder çš„è¾“å…¥å¯¹
        pairs = []
        valid_indices = []
        
        for idx in candidate_indices:
            ics_text = ics_target.iloc[idx]['ICS_Description']
            pairs.append([hs_text, ics_text])
            valid_indices.append(idx)
            
        # æ‰“åˆ†
        rerank_scores = cross_encoder.predict(pairs)
        
        # æ’åº
        scored_candidates = sorted(zip(valid_indices, rerank_scores), key=lambda x: x[1], reverse=True)
        
        # æå– Top K
        top_k_matches = scored_candidates[:FINAL_TOP_K]
        
        match_strs = []
        for idx, score in top_k_matches:
            ics_row = ics_target.iloc[idx]
            match_strs.append(f"[{ics_row['ICS_Code']}] {ics_row['ICS_Description']}")
            
        results.append({
            'HS_Code': hs_target.iloc[i]['HS_Code'],
            'HS_Description': hs_target.iloc[i]['HS_Description'],
            'Context_Used': hs_text,
            'Best_Matches': " | ".join(match_strs)
        })
        
        if (i + 1) % 100 == 0:
            print(f"å·²å¤„ç† {i + 1}/{total} ä¸ªäº§å“...")

    # ==========================================
    # ğŸ’¾ ä¿å­˜ç»“æœ
    # ==========================================
    df_res = pd.DataFrame(results)
    # ä¿®æ”¹ï¼šç›´æ¥ä¿å­˜ä¸º Excel
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ä¸º Excel æ–‡ä»¶: {OUTPUT_FILE} ...")
    df_res.to_excel(OUTPUT_FILE, index=False, engine='openpyxl')
    print(f"\nâœ… ç©¶æåŒ¹é…å®Œæˆï¼ä¸»äººå–µï¼Œè¯·æŸ¥çœ‹æ–‡ä»¶: {OUTPUT_FILE}")

if __name__ == "__main__":
    try:
        run_ultimate_matching()
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"ğŸ™€ å“å‘€ï¼Œè¿è¡Œå‡ºé”™äº†ä¸»äººå–µ: {e}")
