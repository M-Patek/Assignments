import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer, CrossEncoder
import torch
import os
import re

HS_FILE_PATH = "HS07.xlsx - Sheet1.csv"
ICS_FILE_PATH = "Data_ics_ed7.xlsx - Sheet1.csv"
OUTPUT_FILE = "HS_to_ICS_Ultimate_Match.xlsx"

# åˆç­›æ•°é‡
CANDIDATE_POOL_SIZE = 20  
# æœ€ç»ˆä¿ç•™æ•°é‡
FINAL_TOP_K = 3           

# æƒé‡é…ç½®
ALPHA_SEMANTIC = 0.7 
ALPHA_KEYWORD = 0.3

def robust_read_csv(file_path):
    """
    è‡ªåŠ¨å°è¯•å¤šç§ç¼–ç  + å¤šç§åˆ†éš”ç¬¦è¯»å– CSV çš„å·¥å…·
    (å‡çº§ç‰ˆï¼šå¢åŠ äº†â€œé€è§†çœ¼â€ï¼Œä¸“é—¨å¤„ç†æŠŠ xlsx ç›´æ¥æ”¹åä¸º csv çš„æƒ…å†µï¼)
    """
    print(f"ğŸ” æ­£åœ¨å°è¯•è¯»å–æ–‡ä»¶: {file_path} ...")
    
    # === 0. ä¼˜å…ˆå°è¯•ï¼šè¿™æ˜¯å¦æ˜¯ä¼ªè£…æˆ CSV çš„ Excel æ–‡ä»¶ï¼Ÿ ===
    # ä¸»äººè¯´ç›´æ¥æ”¹äº†åç¼€ï¼Œæ‰€ä»¥è¿™å…¶å®æ˜¯ Excel (zip) æ–‡ä»¶ï¼
    try:
        # æˆ‘ä»¬ç”¨äºŒè¿›åˆ¶æ–¹å¼æ‰“å¼€ï¼Œç»•è¿‡åç¼€åæ£€æŸ¥ï¼Œç›´æ¥å–‚ç»™ read_excel
        with open(file_path, 'rb') as f:
            df = pd.read_excel(f, header=None, dtype=str, engine='openpyxl')
            
        print(f"âœ… å‘ç°è¿™å…¶å®æ˜¯ä¸€ä¸ª Excel æ–‡ä»¶ï¼æˆåŠŸè¯»å–: å½¢çŠ¶ {df.shape}")
        return df
    except Exception as e:
        # å¦‚æœä¸æ˜¯ Excelï¼Œæˆ–è€…è¯»å–å¤±è´¥ï¼Œå°±ç»§ç»­å¾€ä¸‹èµ°
        # print(f"   (å¹¶ä¸æ˜¯ Excel æ–‡ä»¶ï¼Œç»§ç»­å°è¯•æ–‡æœ¬æ¨¡å¼...)")
        pass

    # === å¦‚æœä¸Šé¢å¤±è´¥äº†ï¼Œè¯´æ˜å®ƒçœŸçš„æ˜¯ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œå¼€å§‹å°è¯•å„ç§ç¼–ç  ===
    
    # ä¼˜å…ˆçº§è°ƒæ•´ï¼šGB18030 (ä¸­æ–‡) æ’åœ¨æœ€å‰é¢ï¼
    strict_encodings = ['gb18030', 'utf-8-sig', 'utf-8', 'gbk']
    separators = [';', ',', '\t'] 
    
    best_df = None
    max_cols = 0
    
    # === ç¬¬ä¸€è½®ï¼šä¸¥æ ¼æ¨¡å¼ (Strict) ===
    for enc in strict_encodings:
        for sep in separators:
            try:
                df = pd.read_csv(
                    file_path, 
                    header=None, 
                    dtype=str, 
                    encoding=enc, 
                    sep=sep,
                    on_bad_lines='skip', 
                    engine='python'
                )
                if df.shape[1] > max_cols:
                    max_cols = df.shape[1]
                    best_df = df
                    best_df.attrs['encoding_used'] = enc 
                
                if df.shape[1] >= 2:
                    print(f"âœ… [ä¸¥æ ¼æ¨¡å¼] æˆåŠŸè¯»å–! ç¼–ç : {enc}, åˆ†éš”ç¬¦: {repr(sep)}, å½¢çŠ¶: {df.shape}")
                    return df
            except Exception:
                continue

    # === ç¬¬äºŒè½®ï¼šå®¹é”™æ¨¡å¼ (Replace) ===
    print("âš ï¸ ä¸¥æ ¼æ¨¡å¼è¯»å–å¤±è´¥ï¼ŒçŒ«çŒ«å¼€å¯å®¹é”™æ¨¡å¼ï¼ˆå¿½ç•¥ä¸ªåˆ«åå­—ç¬¦ï¼‰...")
    for enc in ['gb18030', 'utf-8-sig', 'latin1']:
        for sep in separators:
            try:
                df = pd.read_csv(
                    file_path, 
                    header=None, 
                    dtype=str, 
                    encoding=enc, 
                    sep=sep,
                    encoding_errors='replace',
                    on_bad_lines='skip', 
                    engine='python'
                )
                if df.shape[1] > max_cols:
                    max_cols = df.shape[1]
                    best_df = df
                
                if df.shape[1] >= 2:
                    print(f"âœ… [å®¹é”™æ¨¡å¼] æˆåŠŸè¯»å–! ç¼–ç : {enc}, åˆ†éš”ç¬¦: {repr(sep)}, å½¢çŠ¶: {df.shape}")
                    return df
            except Exception:
                continue

    # === ç¬¬ä¸‰è½®ï¼šæœ€åæ‰‹æ®µ ===
    if best_df is not None:
        print(f"âš ï¸ è­¦å‘Š: ä½¿ç”¨äº†ä¸å¤ªå®Œç¾çš„è¯»å–æ–¹å¼ (å¯èƒ½å«ä¹±ç )ï¼Œå½¢çŠ¶: {best_df.shape}")
        return best_df

    raise ValueError(f"æ²¡èƒ½è¯»æ‡‚è¿™ä¸ªæ–‡ä»¶çš„æ ¼å¼: {file_path}")

def fix_one_column_df(df, name="æ•°æ®"):
    """
    å¦‚æœæ•°æ®åªæœ‰1åˆ—ï¼Œå°è¯•æ™ºèƒ½ä¿®å¤
    """
    if df.shape[1] >= 2:
        return df
        
    print(f"ğŸ”§ {name} åªæœ‰1åˆ—ï¼ŒçŒ«çŒ«å°è¯•è¿›è¡Œæ™ºèƒ½åˆ†åˆ—...")
    try:
        series = df.iloc[:, 0].astype(str)
        # å¸¸è§åˆ†éš”ç¬¦
        for sep in [';', ',', '\t', ' ']:
            split_df = series.str.split(sep, n=1, expand=True)
            if split_df.shape[1] >= 2:
                print(f"   âœ¨ ä½¿ç”¨ '{sep}' æˆåŠŸæ‹†åˆ†!")
                return split_df
    except Exception:
        pass
        
    print(f"   ğŸ’¨ æ‹†åˆ†å¤±è´¥ï¼Œå°†å¤åˆ¶ç¬¬ä¸€åˆ—ä½œä¸ºç¬¬äºŒåˆ—ä»¥é˜²å´©æºƒ...")
    df['Description_Placeholder'] = df.iloc[:, 0]
    return df

def preprocess_hs_with_context(hs_df):
    """
    ä¸Šä¸‹æ–‡å¢å¼ºå¤„ç†
    """
    print("âœ¨ æ­£åœ¨è¿›è¡Œä¸Šä¸‹æ–‡å¢å¼ºå¤„ç†...")
    
    if hs_df.shape[1] < 2:
        hs_df = fix_one_column_df(hs_df, "HSæ•°æ®")
        
    hs_df = hs_df.iloc[:, :2]
    hs_df.columns = ['HS_Code', 'HS_Description']
    
    # å¼ºåŠ›æ¸…æ´—
    hs_df['HS_Code'] = hs_df['HS_Code'].astype(str).str.replace('.', '', regex=False).str.strip()
    hs_df['HS_Description'] = hs_df['HS_Description'].fillna('').str.strip()
    
    # æå–ç« èŠ‚å’Œæ ‡é¢˜
    chapters = hs_df[hs_df['HS_Code'].str.len() == 2].set_index('HS_Code')['HS_Description'].to_dict()
    headings = hs_df[hs_df['HS_Code'].str.len() == 4].set_index('HS_Code')['HS_Description'].to_dict()

    # åªè¦é•¿åº¦ >= 6 çš„éƒ½ä¿ç•™
    hs_target = hs_df[hs_df['HS_Code'].str.len() >= 6].copy()
    hs_target['HS6_Clean'] = hs_target['HS_Code'].str.slice(0, 6)
    
    enhanced_descriptions = []
    for idx, row in hs_target.iterrows():
        code = row['HS6_Clean']
        desc = row['HS_Description']
        
        chap_code = str(code)[:2]
        head_code = str(code)[:4]
        
        context_str = ""
        if chap_code in chapters:
            context_str += f"{chapters[chap_code]} > "
        if head_code in headings:
            context_str += f"{headings[head_code]} > "
            
        full_desc = f"{context_str}{desc}"
        enhanced_descriptions.append(full_desc)
        
    hs_target['Enhanced_Description'] = enhanced_descriptions
    return hs_target

def run_ultimate_matching():
    print("ğŸ± å¯åŠ¨åŒ¹é…å¼•æ“ (Retrieve & Re-rank)...")
    
    # 1. åŠ è½½æ•°æ®
    hs_df = robust_read_csv(HS_FILE_PATH)
    ics_df = robust_read_csv(ICS_FILE_PATH)
    
    # ä¿®å¤åˆ—æ•°
    if ics_df.shape[1] < 2:
        ics_df = fix_one_column_df(ics_df, "ICSæ•°æ®")
    
    if ics_df.shape[1] >= 3:
         ics_df = ics_df.iloc[:, :3]
         ics_df.columns = ['ICS_Code', 'ICS_Description', 'Finest_Level']
    else:
        print("âš ï¸ è­¦å‘Šï¼šICS æ–‡ä»¶å°‘äº3åˆ—ï¼Œå°è¯•å¼ºåˆ¶è§£æ...")
        ics_df = ics_df.iloc[:, :2]
        ics_df.columns = ['ICS_Code', 'ICS_Description']
        ics_df['Finest_Level'] = '1' 
    
    # 2. ä¸Šä¸‹æ–‡å¢å¼º
    try:
        hs_target = preprocess_hs_with_context(hs_df)
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"âŒ å¤„ç† HS æ•°æ®æ—¶å‡ºé”™: {e}")
        return
    
    # ICS æ¸…æ´—
    ics_df['ICS_Description'] = ics_df['ICS_Description'].fillna('').str.strip()
    ics_target = ics_df[ics_df['Finest_Level'] == '1'].reset_index(drop=True)
    
    if len(ics_target) == 0:
        print("âš ï¸ è­¦å‘Šï¼šç­›é€‰ Finest_Level='1' åä¸ºç©ºï¼Œå°è¯•ä½¿ç”¨æ‰€æœ‰ ICS æ¡ç›®...")
        ics_target = ics_df
    
    if len(ics_target) == 0:
        print("âŒ é”™è¯¯ï¼šICS ç›®æ ‡åº“ä¸ºç©ºï¼Œæ— æ³•è¿›è¡ŒåŒ¹é…ï¼")
        return

    print(f"ğŸ“Š å¾…åŒ¹é… HSæ¡ç›®: {len(hs_target)}")
    print(f"ğŸ“š ç›®æ ‡ ICSåº“: {len(ics_target)}")

    # ==========================================
    # Stage 1: å¿«é€Ÿå¬å›
    # ==========================================
    print("\n[Stage 1] å¿«é€Ÿå¬å› (Bi-Encoder)...")
    bi_encoder = SentenceTransformer('all-MiniLM-L6-v2')
    
    hs_descriptions = hs_target['Enhanced_Description'].tolist()
    ics_descriptions = ics_target['ICS_Description'].tolist()
    
    if not hs_descriptions:
        print("ğŸ™€ HS æ•°æ®åˆ—è¡¨ä¸ºç©ºï¼")
        return

    hs_embeddings = bi_encoder.encode(hs_descriptions, convert_to_tensor=True, show_progress_bar=True)
    ics_embeddings = bi_encoder.encode(ics_descriptions, convert_to_tensor=True, show_progress_bar=True)
    
    semantic_sim = cosine_similarity(hs_embeddings.cpu(), ics_embeddings.cpu())
    
    print("[Stage 1] å…³é”®è¯ä¿®æ­£ (TF-IDF)...")
    tfidf = TfidfVectorizer(stop_words='english')
    corpus = hs_descriptions + ics_descriptions
    tfidf.fit(corpus)
    
    hs_tfidf = tfidf.transform(hs_descriptions)
    ics_tfidf = tfidf.transform(ics_descriptions)
    keyword_sim = cosine_similarity(hs_tfidf, ics_tfidf)
    
    stage1_scores = (semantic_sim * ALPHA_SEMANTIC) + (keyword_sim * ALPHA_KEYWORD)

    # ==========================================
    # Stage 2: ç²¾ç»†é‡æ’åº
    # ==========================================
    print("\n[Stage 2] æ·±åº¦é‡æ’åº (Cross-Encoder)...")
    cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
    
    results = []
    total = len(hs_target)
    
    for i in range(total):
        candidate_indices = stage1_scores[i].argsort()[-CANDIDATE_POOL_SIZE:][::-1]
        hs_text = hs_target.iloc[i]['Enhanced_Description']
        
        pairs = []
        valid_indices = []
        for idx in candidate_indices:
            ics_text = ics_target.iloc[idx]['ICS_Description']
            pairs.append([hs_text, ics_text])
            valid_indices.append(idx)
            
        rerank_scores = cross_encoder.predict(pairs)
        scored_candidates = sorted(zip(valid_indices, rerank_scores), key=lambda x: x[1], reverse=True)
        top_k_matches = scored_candidates[:FINAL_TOP_K]
        
        match_strs = []
        for idx, score in top_k_matches:
            ics_row = ics_target.iloc[idx]
            match_strs.append(f"[{ics_row['ICS_Code']}] {ics_row['ICS_Description']}")
            
        results.append({
            'HS_Code': hs_target.iloc[i]['HS_Code'],
            'HS_Description': hs_target.iloc[i]['HS_Description'],
            'Context_Used': hs_text,
            'Best_Matches': " | ".join(match_strs)
        })
        
        if (i + 1) % 100 == 0:
            print(f"å·²å¤„ç† {i + 1}/{total} ä¸ªäº§å“...")

    # ä¿å­˜
    df_res = pd.DataFrame(results)
    
    # æ¸…æ´—éæ³•å­—ç¬¦
    def clean_illegal_chars(text):
        if isinstance(text, str):
            # ç§»é™¤ä¸å¯è§å­—ç¬¦ (0-31)ï¼Œä¿ç•™ \t \n \r
            return re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f]', '', text)
        return text

    try:
        df_res = df_res.map(clean_illegal_chars)
    except AttributeError:
        df_res = df_res.applymap(clean_illegal_chars)

    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ä¸º Excel æ–‡ä»¶: {OUTPUT_FILE} ...")
    try:
        df_res.to_excel(OUTPUT_FILE, index=False, engine='openpyxl')
        print(f"\nâœ… ç©¶æåŒ¹é…å®Œæˆï¼ä¸»äººå–µï¼Œè¯·æŸ¥çœ‹æ–‡ä»¶: {OUTPUT_FILE}")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜ Excel å¤±è´¥ ({e})ï¼Œå°è¯•ä¿å­˜ CSV...")
        csv_backup = OUTPUT_FILE.replace('.xlsx', '_backup.csv')
        df_res.to_csv(csv_backup, index=False, encoding='utf-8-sig')
        print(f"âœ… å·²ç´§æ€¥ä¿å­˜ä¸º CSV: {csv_backup}")

if __name__ == "__main__":
    try:
        run_ultimate_matching()
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"ğŸ™€ å“å‘€ï¼Œè¿è¡Œå‡ºé”™äº†å–µ: {e}")
