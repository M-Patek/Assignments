import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer, CrossEncoder
import torch

# ==========================================
# âš™ï¸ ç©¶æç‰ˆé…ç½®
# ==========================================
HS_FILE_PATH = "HS07.xlsx - Sheet1.csv"
ICS_FILE_PATH = "Data_ics_ed7.xlsx - Sheet1.csv"
OUTPUT_FILE = "HS_to_ICS_Ultimate_Match.csv"

# åˆç­›æ•°é‡ï¼šç¬¬ä¸€æ­¥å…ˆæ‰¾å›å¤šå°‘ä¸ªå€™é€‰ï¼Ÿ
CANDIDATE_POOL_SIZE = 20  
# æœ€ç»ˆä¿ç•™æ•°é‡
FINAL_TOP_K = 3           

# æƒé‡é…ç½®ï¼ˆåˆç­›é˜¶æ®µï¼‰
ALPHA_SEMANTIC = 0.7 
ALPHA_KEYWORD = 0.3

def preprocess_hs_with_context(hs_df):
    """
    ä¸Šä¸‹æ–‡å¢å¼ºï¼šå°† HS çš„ç« èŠ‚æ ‡é¢˜ (2ä½ç¼–ç ) æ‹¼æ¥åˆ° 6ä½ç¼–ç æè¿°å‰ã€‚
    è§£å†³å¾ˆå¤š 6ä½ç¼–ç æè¿°åªæ˜¯ "Other" æˆ– "Parts" çš„é—®é¢˜ã€‚
    """
    print("âœ¨ æ­£åœ¨è¿›è¡Œä¸Šä¸‹æ–‡å¢å¼ºå¤„ç†...")
    hs_df['HS_Code'] = hs_df['HS_Code'].str.strip()
    hs_df['HS_Description'] = hs_df['HS_Description'].fillna('').str.strip()
    
    # æå– 2ä½æ•° ç« èŠ‚ (Chapter) åŠå…¶æè¿°
    # å‡è®¾ 2ä½æ•°çš„è¡Œå…¶ HS_Code é•¿åº¦ä¸º 2
    chapters = hs_df[hs_df['HS_Code'].str.len() == 2].set_index('HS_Code')['HS_Description'].to_dict()
    
    # æå– 4ä½æ•° (Heading) åŠå…¶æè¿° (å¯é€‰ï¼Œä¸ºäº†æ›´ç²¾å‡†å¯ä»¥åŠ ä¸Š)
    headings = hs_df[hs_df['HS_Code'].str.len() == 4].set_index('HS_Code')['HS_Description'].to_dict()

    # ç­›é€‰ç›®æ ‡ 6ä½æ•°äº§å“
    hs_target = hs_df[hs_df['HS_Code'].str.len() == 6].copy()
    
    enhanced_descriptions = []
    for idx, row in hs_target.iterrows():
        code = row['HS_Code']
        desc = row['HS_Description']
        
        # æŸ¥æ‰¾çˆ¶çº§
        chap_code = code[:2]
        head_code = code[:4]
        
        context_str = ""
        if chap_code in chapters:
            context_str += f"{chapters[chap_code]} > "
        if head_code in headings:
            # æœ‰äº› heading æè¿°å¤ªé•¿ï¼Œå¯ä»¥æˆªæ–­ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ç›´æ¥æ‹¼æ¥
            context_str += f"{headings[head_code]} > "
            
        # æ‹¼æ¥æœ€ç»ˆæè¿°ï¼š [ç« èŠ‚] > [æ ‡é¢˜] > [å­ç›®]
        full_desc = f"{context_str}{desc}"
        enhanced_descriptions.append(full_desc)
        
    hs_target['Enhanced_Description'] = enhanced_descriptions
    return hs_target

def run_ultimate_matching():
    print("ğŸ± å¯åŠ¨ç©¶æåŒ¹é…å¼•æ“ (Retrieve & Re-rank)...")
    
    # 1. åŠ è½½æ•°æ®
    hs_df = pd.read_csv(HS_FILE_PATH, header=None, dtype=str)
    ics_df = pd.read_csv(ICS_FILE_PATH, header=None, dtype=str)
    
    hs_df.columns = ['HS_Code', 'HS_Description']
    ics_df.columns = ['ICS_Code', 'ICS_Description', 'Finest_Level']
    
    # 2. ä¸Šä¸‹æ–‡å¢å¼º (HSç«¯)
    hs_target = preprocess_hs_with_context(hs_df)
    
    # ICSç«¯å¤„ç†
    ics_df['ICS_Description'] = ics_df['ICS_Description'].fillna('').str.strip()
    ics_target = ics_df[ics_df['Finest_Level'] == '1'].reset_index(drop=True)
    
    print(f"ğŸ“Š å¾…åŒ¹é… HSæ¡ç›®: {len(hs_target)} (å·²å¢å¼ºä¸Šä¸‹æ–‡)")
    print(f"ğŸ“š ç›®æ ‡ ICSåº“: {len(ics_target)}")

    # ==========================================
    # ğŸš€ Stage 1: å¿«é€Ÿå¬å› (Bi-Encoder + TF-IDF)
    # ==========================================
    print("\n[Stage 1] å¿«é€Ÿå¬å› (Bi-Encoder)...")
    
    # åŠ è½½ Bi-Encoder æ¨¡å‹
    bi_encoder = SentenceTransformer('all-MiniLM-L6-v2')
    
    # ç¼–ç 
    # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ 'Enhanced_Description' è¿›è¡ŒåŒ¹é…ï¼Œä¿¡æ¯é‡æ›´å¤§
    hs_embeddings = bi_encoder.encode(hs_target['Enhanced_Description'].tolist(), convert_to_tensor=True, show_progress_bar=True)
    ics_embeddings = bi_encoder.encode(ics_target['ICS_Description'].tolist(), convert_to_tensor=True, show_progress_bar=True)
    
    # è¯­ä¹‰ç›¸ä¼¼åº¦
    semantic_sim = cosine_similarity(hs_embeddings.cpu(), ics_embeddings.cpu())
    
    # TF-IDF è¾…åŠ© (é’ˆå¯¹å…³é”®è¯)
    print("[Stage 1] å…³é”®è¯ä¿®æ­£ (TF-IDF)...")
    tfidf = TfidfVectorizer(stop_words='english')
    # è®­ç»ƒé›†åŒ…å«ä¸¤è€…
    corpus = hs_target['Enhanced_Description'].tolist() + ics_target['ICS_Description'].tolist()
    tfidf.fit(corpus)
    
    hs_tfidf = tfidf.transform(hs_target['Enhanced_Description'])
    ics_tfidf = tfidf.transform(ics_target['ICS_Description'])
    keyword_sim = cosine_similarity(hs_tfidf, ics_tfidf)
    
    # æ··åˆåˆ†æ•°
    stage1_scores = (semantic_sim * ALPHA_SEMANTIC) + (keyword_sim * ALPHA_KEYWORD)

    # ==========================================
    # ğŸ’ Stage 2: ç²¾ç»†é‡æ’åº (Cross-Encoder)
    # ==========================================
    print("\n[Stage 2] æ·±åº¦é‡æ’åº (Cross-Encoder)... æ­¤æ­¥éª¤è¾ƒæ…¢ï¼Œä½†æœ€å‡†ï¼")
    
    # åŠ è½½ Cross-Encoder æ¨¡å‹
    # ms-marco-MiniLM-L-6-v2 æ˜¯ä¸“é—¨è®­ç»ƒæ¥åˆ¤æ–­ "è¿™ä¸¤ä¸ªå¥å­æ˜¯å¦ç›¸å…³" çš„æ¨¡å‹
    cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
    
    results = []
    
    # éå†æ¯ä¸ª HS äº§å“
    total = len(hs_target)
    for i in range(total):
        # 1. è·å– Stage 1 åˆ†æ•°æœ€é«˜çš„ Top N ä¸ªå€™é€‰è€…çš„ç´¢å¼•
        # argsortæ˜¯ä»å°åˆ°å¤§ï¼Œå–æœ€å pool_size ä¸ªï¼Œå†åè½¬
        candidate_indices = stage1_scores[i].argsort()[-CANDIDATE_POOL_SIZE:][::-1]
        
        hs_text = hs_target.iloc[i]['Enhanced_Description']
        
        # 2. å‡†å¤‡ Cross-Encoder çš„è¾“å…¥å¯¹
        # æ ¼å¼: [[HSæ–‡æœ¬, ICSå€™é€‰1], [HSæ–‡æœ¬, ICSå€™é€‰2], ...]
        pairs = []
        valid_indices = [] # è®°å½•å¯¹åº”çš„ ICS ç´¢å¼•
        
        for idx in candidate_indices:
            ics_text = ics_target.iloc[idx]['ICS_Description']
            pairs.append([hs_text, ics_text])
            valid_indices.append(idx)
            
        # 3. Cross-Encoder æ‰“åˆ† (é¢„æµ‹ Logits)
        rerank_scores = cross_encoder.predict(pairs)
        
        # 4. æ’åºæœ€ç»ˆç»“æœ
        # å°†åˆ†æ•°å’Œå¯¹åº”çš„ ICS ç´¢å¼•æ‰“åŒ…
        scored_candidates = list(zip(valid_indices, rerank_scores))
        # æŒ‰åˆ†æ•°é™åºæ’åˆ—
        scored_candidates.sort(key=lambda x: x[1], reverse=True)
        
        # 5. æå–å‰ Top K
        top_k_matches = scored_candidates[:FINAL_TOP_K]
        
        match_strs = []
        for idx, score in top_k_matches:
            # Sigmoid å°† logit è½¬ä¸º 0-1 çš„æ¦‚ç‡æ„Ÿ (å¯é€‰ï¼Œè¿™é‡Œç›´æ¥ç”¨ raw score ä¹Ÿè¡Œ)
            # ä¸ºäº†ç›´è§‚ï¼Œæˆ‘ä»¬åªå±•ç¤º Code å’Œ Desc
            ics_row = ics_target.iloc[idx]
            match_strs.append(f"[{ics_row['ICS_Code']}] {ics_row['ICS_Description']}")
            
        results.append({
            'HS_Code': hs_target.iloc[i]['HS_Code'],
            'HS_Description': hs_target.iloc[i]['HS_Description'], # åŸå§‹æè¿°
            'Context_Used': hs_text, # å¢å¼ºåçš„æè¿° (æ–¹ä¾¿æ ¸å¯¹)
            'Best_Matches': " | ".join(match_strs)
        })
        
        if (i + 1) % 100 == 0:
            print(f"å·²ç²¾ä¿® {i + 1}/{total} ä¸ªäº§å“...")

    # ==========================================
    # ğŸ’¾ ä¿å­˜ç»“æœ
    # ==========================================
    df_res = pd.DataFrame(results)
    df_res.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')
    print(f"\nâœ… ç©¶æåŒ¹é…å®Œæˆï¼æ–‡ä»¶å·²ä¿å­˜: {OUTPUT_FILE}")

if __name__ == "__main__":
    run_ultimate_matching()
